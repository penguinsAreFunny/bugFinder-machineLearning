{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\anaconda3\\envs\\ml\\lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sweetviz as sv\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tpot import TPOTClassifier \n",
    "\n",
    "import models.DecisionTreeClassifier as dtc\n",
    "import models.RandomForestClassifier as rfc\n",
    "import models.SupportVectorMachine as spm\n",
    "import models.DummyClassifiers as dummies\n",
    "import feature_reduction.PCA as pca_wrapper\n",
    "import models.Evaluation as eval\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reads dataset.json-file and parses it to an obj\n",
    "def readFile(filepath):\n",
    "    file = open(filepath, \"r\")\n",
    "    content = file.read()\n",
    "    json_content = json.loads(content)\n",
    "    print(json_content.keys())\n",
    "    #print(json_content[\"data\"])\n",
    "    return dotdict(json_content)\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'keys', 'featureNames', 'targetNames', 'description', 'traceAP'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fixes</th>\n",
       "      <th>minValCognitiveComplexity</th>\n",
       "      <th>minValDuplicatedLinesDensity</th>\n",
       "      <th>minValSecurityRating</th>\n",
       "      <th>minValBlockerViolations</th>\n",
       "      <th>minValDuplicatedBlocks</th>\n",
       "      <th>minValVulnerabilities</th>\n",
       "      <th>minValSqaleIndex</th>\n",
       "      <th>minValInfoViolations</th>\n",
       "      <th>minValGeneratedNcloc</th>\n",
       "      <th>...</th>\n",
       "      <th>meanRelDiffTestSuccessDensity</th>\n",
       "      <th>meanRelDiffFalsePositiveIssues</th>\n",
       "      <th>meanRelDiffTestFailures</th>\n",
       "      <th>meanRelDiffSqaleRating</th>\n",
       "      <th>meanRelDiffReliabilityRating</th>\n",
       "      <th>meanRelDiffFiles</th>\n",
       "      <th>meanRelDiffWontFixIssues</th>\n",
       "      <th>meanRelDiffSkippedTests</th>\n",
       "      <th>meanRelDiffEffortToReachMaintainabilityRatingA</th>\n",
       "      <th>meanRelDiffTestErrors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9858.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11188.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fixes  minValCognitiveComplexity  minValDuplicatedLinesDensity  \\\n",
       "0    0.0                      290.0                           0.0   \n",
       "1    0.0                      456.0                           0.0   \n",
       "2    0.0                     9858.0                           0.0   \n",
       "3    0.0                        0.0                           0.0   \n",
       "4    0.0                        0.0                           0.0   \n",
       "\n",
       "   minValSecurityRating  minValBlockerViolations  minValDuplicatedBlocks  \\\n",
       "0                   1.0                      0.0                     0.0   \n",
       "1                   1.0                      0.0                     0.0   \n",
       "2                   1.0                      3.0                     0.0   \n",
       "3                   1.0                      0.0                     0.0   \n",
       "4                   1.0                      0.0                     0.0   \n",
       "\n",
       "   minValVulnerabilities  minValSqaleIndex  minValInfoViolations  \\\n",
       "0                    0.0              59.0                   0.0   \n",
       "1                    0.0              55.0                   0.0   \n",
       "2                    0.0           11188.0                   8.0   \n",
       "3                    0.0               0.0                   0.0   \n",
       "4                    0.0               0.0                   0.0   \n",
       "\n",
       "   minValGeneratedNcloc  ...  meanRelDiffTestSuccessDensity  \\\n",
       "0                   0.0  ...                            0.0   \n",
       "1                   0.0  ...                            0.0   \n",
       "2                   0.0  ...                            0.0   \n",
       "3                   0.0  ...                            0.0   \n",
       "4                   0.0  ...                            0.0   \n",
       "\n",
       "   meanRelDiffFalsePositiveIssues  meanRelDiffTestFailures  \\\n",
       "0                             0.0                      0.0   \n",
       "1                             0.0                      0.0   \n",
       "2                             0.0                      0.0   \n",
       "3                             0.0                      0.0   \n",
       "4                             0.0                      0.0   \n",
       "\n",
       "   meanRelDiffSqaleRating  meanRelDiffReliabilityRating  meanRelDiffFiles  \\\n",
       "0                     0.0                      0.000000               0.0   \n",
       "1                     0.0                      0.333333               0.0   \n",
       "2                     0.0                      0.000000               0.0   \n",
       "3                     0.0                      0.000000               0.0   \n",
       "4                     0.0                      0.000000               0.0   \n",
       "\n",
       "   meanRelDiffWontFixIssues  meanRelDiffSkippedTests  \\\n",
       "0                       0.0                      0.0   \n",
       "1                       0.0                      0.0   \n",
       "2                       0.0                      0.0   \n",
       "3                       0.0                      0.0   \n",
       "4                       0.0                      0.0   \n",
       "\n",
       "   meanRelDiffEffortToReachMaintainabilityRatingA  meanRelDiffTestErrors  \n",
       "0                                             0.0                    0.0  \n",
       "1                                             0.0                    0.0  \n",
       "2                                             0.0                    0.0  \n",
       "3                                             0.0                    0.0  \n",
       "4                                             0.0                    0.0  \n",
       "\n",
       "[5 rows x 234 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_file_path = \"./dataset_5_5.json\"\n",
    "dataset = readFile(dataset_file_path)\n",
    "df = pd.DataFrame(data= np.c_[dataset.target, dataset.data], columns=dataset.targetNames + dataset.featureNames)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Preparing data before using it to train different models\n",
    "## Standardization\n",
    "Standardizing data. Some models are sensitive against outlier values. <br>\n",
    "They perform better with standardized data. <br>\n",
    "Model that might benefit from standardization: Polynomial Regression <br>\n",
    "## Feature Reduction - \"Curse of dimensionality\"\n",
    "Some models have poor training performance. A lot of features might lead to very high training times.<br>\n",
    "Usually it is best to keep as many features as suitable.<br>\n",
    "Model that has poor training performance: Support Vector Machines<br>\n",
    "### PCA - Principal Component Analysis\n",
    "PCA is a feature reduction algorithm which tries to reduce all features into less features while keeping as much<br>\n",
    "variance of the data as possible. The idea is that most information is kept in most of the variance ratio.<br>\n",
    "Concept: If you have 2 Dimensions (x- + y- axis) you can plot all points into that grid and try to plot one new axis<br>\n",
    "and measure the distances of the points to the axis. Therefore you stay with one feature left.<br>\n",
    "Now you do not know what the feature´s semantic is though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of Explained Variance Ratio: 0 with: 0 components.\n",
      "Sum of Explained Variance Ratio: 0.2339 with: 1 components.\n",
      "Sum of Explained Variance Ratio: 0.3448 with: 2 components.\n",
      "Sum of Explained Variance Ratio: 0.4175 with: 3 components.\n",
      "Sum of Explained Variance Ratio: 0.4749 with: 4 components.\n",
      "Sum of Explained Variance Ratio: 0.5244 with: 5 components.\n",
      "Sum of Explained Variance Ratio: 0.5728 with: 6 components.\n",
      "Sum of Explained Variance Ratio: 0.6099 with: 7 components.\n",
      "Sum of Explained Variance Ratio: 0.6439 with: 8 components.\n",
      "Sum of Explained Variance Ratio: 0.6725 with: 9 components.\n",
      "Sum of Explained Variance Ratio: 0.6968 with: 10 components.\n",
      "Sum of Explained Variance Ratio: 0.7193 with: 11 components.\n",
      "Sum of Explained Variance Ratio: 0.74 with: 12 components.\n",
      "Sum of Explained Variance Ratio: 0.7595 with: 13 components.\n",
      "Sum of Explained Variance Ratio: 0.7773 with: 14 components.\n",
      "Sum of Explained Variance Ratio: 0.7925 with: 15 components.\n",
      "Sum of Explained Variance Ratio: 0.8073 with: 16 components.\n",
      "Sum of Explained Variance Ratio: 0.8212 with: 17 components.\n",
      "Sum of Explained Variance Ratio: 0.8337 with: 18 components.\n",
      "Sum of Explained Variance Ratio: 0.8455 with: 19 components.\n",
      "Sum of Explained Variance Ratio: 0.8561 with: 20 components.\n",
      "Sum of Explained Variance Ratio: 0.8662 with: 21 components.\n",
      "Sum of Explained Variance Ratio: 0.8763 with: 22 components.\n",
      "Sum of Explained Variance Ratio: 0.885 with: 23 components.\n",
      "Sum of Explained Variance Ratio: 0.8936 with: 24 components.\n",
      "Sum of Explained Variance Ratio: 0.9011 with: 25 components.\n",
      "Sum of Explained Variance Ratio: 0.9083 with: 26 components.\n",
      "Sum of Explained Variance Ratio: 0.9152 with: 27 components.\n",
      "Sum of Explained Variance Ratio: 0.9215 with: 28 components.\n",
      "Sum of Explained Variance Ratio: 0.9275 with: 29 components.\n",
      "Sum of Explained Variance Ratio: 0.9327 with: 30 components.\n",
      "Sum of Explained Variance Ratio: 0.9375 with: 31 components.\n",
      "Sum of Explained Variance Ratio: 0.9422 with: 32 components.\n",
      "Sum of Explained Variance Ratio: 0.9464 with: 33 components.\n",
      "Sum of Explained Variance Ratio: 0.9505 with: 34 components.\n",
      "Sum of Explained Variance Ratio: 0.9544 with: 35 components.\n",
      "Sum of Explained Variance Ratio: 0.958 with: 36 components.\n",
      "Sum of Explained Variance Ratio: 0.9612 with: 37 components.\n",
      "Sum of Explained Variance Ratio: 0.964 with: 38 components.\n",
      "Sum of Explained Variance Ratio: 0.9667 with: 39 components.\n",
      "Sum of Explained Variance Ratio: 0.9693 with: 40 components.\n",
      "Sum of Explained Variance Ratio: 0.9716 with: 41 components.\n",
      "Sum of Explained Variance Ratio: 0.9737 with: 42 components.\n",
      "Sum of Explained Variance Ratio: 0.9758 with: 43 components.\n",
      "Sum of Explained Variance Ratio: 0.9776 with: 44 components.\n",
      "Sum of Explained Variance Ratio: 0.9792 with: 45 components.\n",
      "Sum of Explained Variance Ratio: 0.9808 with: 46 components.\n",
      "Explained Variance:\n",
      "[28.06609127 13.31260596  8.72467253  6.88546994  5.93924676  5.81284107\n",
      "  4.45349741  4.08524789  3.42186223  2.92348575  2.69517936  2.48567745\n",
      "  2.33789289  2.14486523  1.8187857   1.78072484  1.6657808   1.49538767\n",
      "  1.42241218  1.26414889  1.2227435   1.20871007  1.04437578  1.0287537\n",
      "  0.89777288  0.86842273  0.82663017  0.75926019  0.71844284  0.61890509\n",
      "  0.58343773  0.56332627  0.50358127  0.48642596  0.47488292  0.42523526\n",
      "  0.38323565  0.33794764  0.32380254  0.31492931  0.27588315  0.25606406\n",
      "  0.25153416  0.21812496  0.19178916  0.18635071]\n",
      "Explained Variance Ratio:\n",
      "[0.23386299 0.11092837 0.07269904 0.05737374 0.04948926 0.04843597\n",
      " 0.03710913 0.03404066 0.02851294 0.02436018 0.0224578  0.02071211\n",
      " 0.01948068 0.01787226 0.01515518 0.01483803 0.01388025 0.01246044\n",
      " 0.01185237 0.01053362 0.01018861 0.01007167 0.00870235 0.00857217\n",
      " 0.00748077 0.0072362  0.00688796 0.0063266  0.00598648 0.00515708\n",
      " 0.00486154 0.00469396 0.00419613 0.00405318 0.003957   0.00354331\n",
      " 0.00319334 0.00281598 0.00269811 0.00262417 0.00229882 0.00213367\n",
      " 0.00209593 0.00181754 0.0015981  0.00155278]\n",
      "Sum of Exmplained Variance Ratio:\n",
      "0.9807985015507522\n",
      "n_components: 46\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "x = np.array(dataset.data[:])\n",
    "y = (np.array(dataset.target)).ravel() # make 1d-Array\n",
    "\n",
    "# Standardized\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset.data)\n",
    "x_standardized = scaler.transform(x)\n",
    "\n",
    "# PCA \n",
    "x_pca = pca_wrapper.pca(x_standardized, 0.98)\n",
    "\n",
    "# Split into train and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "x_train_standardized, x_test_standardized, y_train_standardized, y_test_standardized = train_test_split(x_standardized, y, test_size=0.30, random_state=42)\n",
    "x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_pca, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Generate DataFrame from test and train set as it is needed for h2o \n",
    "df_train = pd.DataFrame(data=np.c_[y_train, x_train], columns=dataset.targetNames + dataset.featureNames)\n",
    "df_test = pd.DataFrame(data=np.c_[y_test, x_test], columns=dataset.targetNames + dataset.featureNames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple data analysis\n",
    "Next two cells! <br>\n",
    "Sometimes it might be usefull to explore data before modelling.<br>\n",
    "Some features might obviously have high predictive power others don´t.<br>\n",
    "Simple analysis: Plotting 1 feature vs corresponding classes. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run next cell, too!\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(unique, counts) = np.unique(y, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(\"Class occurences:\\n\", frequencies, \"\\n\")\n",
    "my_report = sv.analyze(df, pairwise_analysis=\"on\")\n",
    "my_report.show_html()\n",
    "\n",
    "\n",
    "minValLines = x[:,9]\n",
    "maxValLines = x[:,37]\n",
    "meanDiffLines = x[:,152]\n",
    "meanRelDiffLines = x[:,219]\n",
    "\n",
    "for idx, val in enumerate(dataset.featureNames):\n",
    "    if (val == \"maxValLines\"):\n",
    "        print(\"maxValLines: \", idx)\n",
    "    if(val == \"minValLines\"):\n",
    "        print(\"minValLines \", idx)\n",
    "    if(val == \"meanRelDiffLines\"):\n",
    "        print(\"meanRelDiffLines \", idx)\n",
    "    if(val == \"meanDiffLines\"):\n",
    "        print(\"meanDiffLines \", idx)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.tight_layout(pad=2.0)\n",
    "ylabel = \"#fixes\"\n",
    "axs[0, 0].scatter(minValLines, y)\n",
    "axs[0, 0].set(xlabel=\"minValLines\")\n",
    "axs[0, 0].set(ylabel=ylabel)\n",
    "\n",
    "axs[0, 1].scatter(maxValLines, y)\n",
    "axs[0, 1].set(xlabel=\"maxValLines\")\n",
    "axs[0, 1].set(ylabel=ylabel)\n",
    "\n",
    "axs[1, 0].scatter(meanDiffLines, y)\n",
    "axs[1, 0].set(xlabel=\"meanDiffLines\")\n",
    "axs[1, 0].set(ylabel=ylabel)\n",
    "\n",
    "axs[1, 1].scatter(meanRelDiffLines, y)\n",
    "axs[1, 1].set(xlabel=\"meanRelDiffLines\")\n",
    "axs[1, 1].set(ylabel=ylabel)\n",
    "\n",
    "image_format = \"svg\"\n",
    "image_name = \"minValLines-fixes.svg\"\n",
    "fig.savefig(image_name, format=image_format, dpi=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "Simple model. Most likely not the best performing model.\n",
    "Tries to split classes based on a decision tree of the features.\n",
    "Why using Decision Trees if they tend to perform bad?\n",
    "You can plot Decision Trees and make them human readable. \n",
    "If your decision tree performs great, plot it and understand what decisions lead to which classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "clf = dtc.model(x_train, y_train)\n",
    "eval.evaluate(clf, x_test, y_test, dataset.featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "Ensemble: Bagging of DecisionTreeClassifier.\n",
    "Train many trees with different features and calculate mean value in case of regression\n",
    "or use majority voter in case of classification.\n",
    "RandomForests tend to overfitting especially when using a lot of trees. \n",
    "Therefore better performance achieved with train-data might lead to worse performance with test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search: finding best hyperparameter for RandomForestClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 160 out of 160 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of best model: {'criterion': 'entropy', 'max_depth': None, 'n_estimators': 160}\n",
      "Score of best model: 0.8223340538742934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Robert\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Robert\\anaconda3\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90      2561\n",
      "           1       0.62      0.34      0.44       643\n",
      "           2       0.42      0.15      0.22       109\n",
      "           3       0.40      0.20      0.27        10\n",
      "           4       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81      3324\n",
      "   macro avg       0.46      0.33      0.36      3324\n",
      "weighted avg       0.78      0.81      0.78      3324\n",
      "\n",
      "\n",
      "x-axis: actual values, y-axis: predicted values\n",
      "[[2464   95    2    0    0]\n",
      " [ 407  218   18    0    0]\n",
      " [  51   39   16    3    0]\n",
      " [   4    2    2    2    0]\n",
      " [   1    0    0    0    0]]\n",
      "Mean absolute error: 0.2075812274368231\n",
      "Top 5 most important features:\n",
      "\t meanValLines   0.038817584047811965\n",
      "\t maxValLines   0.03428578450762277\n",
      "\t minValLines   0.03317183436171333\n",
      "\t meanRelDiffLines   0.032700529576334576\n",
      "\t maxRelDiffLines   0.032250015662894774\n",
      "\n",
      "Most important features: x-Axis: feature, y-Axis: importance\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "clf = rfc.model(x_train, y_train) # if weird OS bug occures: n_jobs = -1 might not be working\n",
    "eval.evaluate(clf, x_test, y_test, dataset.featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "Support Vector Machines can make good models but need more computing time to fit data.<br>\n",
    "Therefore it might be necessary to use feature-reduction before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "clf = spm.model(x_train_pca, y_train_pca)\n",
    "eval.evaluate(clf, x_test_pca, y_test_pca, dataset.featureNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated machine learning\n",
    "There are different approaches to automize machine learning. <br>\n",
    "One of which is **T**ree-based **P**ipeline **O**ptimization **T**ool which optimizes machine learning pipelines using genetic programming. <br>\n",
    "There are other automated machine learning tools like Azure AutoML and Google Cloud AutoML which need further investigation.<br>\n",
    "Investigate https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT http://epistasislab.github.io/tpot/using/\n",
    "tpot = TPOTClassifier(generations=100, verbosity=2, memory=\"./tpot-memory\")\n",
    "tpot.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export(\"./tpot_test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>3 hours 50 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.34.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>30 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Robert_2hi6yh</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.580 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.12 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------------\n",
       "H2O_cluster_uptime:         3 hours 50 mins\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.34.0.3\n",
       "H2O_cluster_version_age:    30 days\n",
       "H2O_cluster_name:           H2O_from_python_Robert_2hi6yh\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.580 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.8.12 final\n",
       "--------------------------  ---------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "23:57:19.860: AutoML: XGBoost is not available; skipping it.\n",
      "23:57:19.860: Step 'best_of_family_xgboost' not defined in provider 'StackedEnsemble': skipping it.\n",
      "23:57:19.860: Step 'all_xgboost' not defined in provider 'StackedEnsemble': skipping it.\n",
      "\n",
      "██████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16996/98637354.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mh2o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"./models/trained_model_dumps/h2o.model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# h2o\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from numpy.random import choice\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "h2o_train = h2o.H2OFrame(df_train)\n",
    "h2o_test = h2o.H2OFrame(df_test)\n",
    "# train, test = h2o_df.split_frame(ratios=[.75])\n",
    "\n",
    "target_name = \"Fixes\"\n",
    "x = h2o_train.columns\n",
    "x.remove(target_name)\n",
    "\n",
    "# needed for classification. If not set like that it will go for regression\n",
    "h2o_train[target_name] = h2o_train[target_name].asfactor()\n",
    "h2o_test[target_name] = h2o_test[target_name].asfactor()\n",
    "\n",
    "# If you run with project_name and different seeds new Models will be added to existing leaderboard\n",
    "# Still not ideal for rerunning as same models might be exploited again\n",
    "aml = H2OAutoML(max_runtime_secs=60*60*5, project_name=\"AutoML_h2o\",seed=1)\n",
    "aml.train(x=x, y=target_name, training_frame=h2o_train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n",
    "\n",
    "# Save the model\n",
    "h2o.save_model(model=aml.leader, path=\"./models/trained_model_dumps/h2o.model\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf prediction progress: |███████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16996/295540405.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# for each feature samle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnumpy_y_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh2o_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0meval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mh2o_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ma-implementation-3\\implementation\\src\\04a-training\\src\\python\\models\\Evaluation.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_test, y_pred)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# x-Achse: Actual Values, y-Achse: Predicted Values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = aml.leader.predict(h2o_test)\n",
    "numpy_predictions = predictions.as_data_frame().to_numpy()\n",
    "predictions_class = predictions[\"predict\"].as_data_frame().to_numpy()\n",
    "\n",
    "# h2o model predicts probabilities for each class in training set. Therefore 4 values are predicted\n",
    "# for each feature samle\n",
    "numpy_y_test = h2o_test[target_name].as_data_frame().to_numpy()\n",
    "eval.classification_report(numpy_y_test, predictions_class)\n",
    "print(aml.leader.confusion_matrix(data=h2o_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "old_parameters = {\n",
    "    \"loss\": [\"deviance\", \"exponential\"],\n",
    "    'n_estimators': [10, 20, 40],\n",
    "    'criterion': ['mse', 'mae'],\n",
    "    'max_depth': [None, 2, 4, 8]\n",
    "}\n",
    "parameters = {\n",
    "    \"loss\": [\"deviance\", \"exponential\"],\n",
    "    'n_estimators': [40],\n",
    "    'criterion': ['friedman_mse'],\n",
    "    'max_depth': [None, 10]\n",
    "}\n",
    "\n",
    "clf = GradientBoostingClassifier(verbose=1)\n",
    "grid_cv = GridSearchCV(clf, parameters, cv=10, n_jobs=-1)\n",
    "grid_cv.fit(x_train, y_train)\n",
    "print(f\"Parameters of best model: {grid_cv.best_params_}\")\n",
    "print(f\"Score of best model: {grid_cv.best_score_}\")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=grid_cv.best_params_[\"n_estimators\"], criterion=grid_cv.best_params_[\"criterion\"], max_depth=grid_cv.best_params_[\"max_depth\"]) \n",
    "clf.fit(x_train, y_train)\n",
    "'''\n",
    "y_pred = clf.predict(x_test)\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(report)\n",
    "print(cm)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines and Feature Reduction PCA\n",
    "\n",
    "# Feature Reduction - PCA\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset.data)\n",
    "x_standardized = scaler.transform(x)\n",
    "\n",
    "# Finde die Anzahl an Dimensionen, um 95% der Varianz \"zu erklären\"\n",
    "for n_components in range(1, 120):\n",
    "    pca = PCA(n_components=n_components, copy=True)\n",
    "    pca.fit(x_standardized)\n",
    "    explained_variance_ratio = sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    print(f\"Sum of Explained Variance Ratio: {round(explained_variance_ratio, 4)} with: {n_components} components.\")\n",
    "\n",
    "    if explained_variance_ratio > 0.95:\n",
    "        break\n",
    "    else:  \n",
    "        best_explained_variance_ratio = explained_variance_ratio\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "n_components = 34\n",
    "pca = PCA(n_components=n_components, copy=True)\n",
    "pca.fit(x_standardized)\n",
    "x_pca = pca.transform(x_standardized)\n",
    "print(f\"Explained Variance:\\n{pca.explained_variance_}\")\n",
    "print(f\"Explained Variance Ratio:\\n{pca.explained_variance_ratio_}\")\n",
    "print(f\"Sum of Exmplained Variance Ratio:\\n{sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.30)\n",
    "\n",
    "\n",
    "# Kernel: rbf, linear, poly, sigmoid\n",
    "# C: low <=> smooth dec. boundary, high <=> acc. more important\n",
    "# gamma: low <=> higher influence, high <=> lower influence\n",
    "kernel = \"poly\"\n",
    "C = 0.5\n",
    "gamma = 1 / len(dataset.featureNames)\n",
    "\n",
    "# GridSearch\n",
    "parameters = {\n",
    "    'kernel': ['kernel', 'rbf', 'linear', 'poly', 'sigmoid'],\n",
    "    'gamma': [1 / len(dataset.featureNames)],\n",
    "    'C': [0.2, 0.5, 0.7],\n",
    "}\n",
    "\n",
    "clf = SVC(verbose=1)\n",
    "grid_cv = GridSearchCV(clf, parameters, cv=10, n_jobs=-1)\n",
    "grid_cv.fit(x_train, y_train)\n",
    "\n",
    "# Train with best hyperparameters\n",
    "clf = SVC(kernel=grid_cv.best_params_[\"kernel\"], C=grid_cv.best_params_[\"C\"], gamma=grid_cv.best_params_[\"gamma\"], verbose=True)\n",
    "y_train_flat = [item for sublist in y_train for item in sublist]\n",
    "clf.fit(x_train, y_train_flat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: generates predictions by respecting the training set’s class distribution.\n",
    "clf = DummyClassifier(strategy=\"stratified\")\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Classifiers\n",
    "You should compare your best classifier with some simple and \"stupid\" strategies <br>\n",
    "to make sure your classifier is somewhat better than a very simple classification strategy. <br>\n",
    "You might want to run the next cell twice, if you get warning and cannot see the whole output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me twice if you get warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"WARNING: Warnings will be ignored. Some metrics might be ill defined \" + \n",
    "    \"as no prediction of certain classes might lead to cero division. \" + \n",
    "    \"Please interpret metrics with causion!\")\n",
    "\n",
    "dummy_models = dummies.models(x_train, y_train)\n",
    "for idx, val in enumerate(dummy_models):\n",
    "    print(val.strategy)\n",
    "    eval.evaluate(val.model, x_test, y_test, dataset.featureNames)\n",
    "    print(\"\\n\")\n",
    "\n",
    "warnings.filterwarnings(\"always\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: stratified.\n",
    "clf = DummyClassifier(strategy=\"stratified\")\n",
    "clf.fit(x_train, y_train)\n",
    "eval.evaluate(clf, x_test, y_test, dataset.featureNames)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: always predicts the most frequent label in the training set.\n",
    "clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: always predicts the class that maximizes the class prior (like “most_frequent”) and predict_proba returns the class prior.\n",
    "clf = DummyClassifier(strategy=\"prior\")\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: generates predictions uniformly at random.\n",
    "clf = DummyClassifier(strategy=\"uniform\")\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "eval.evaluate(clf, x_test, y_test, dataset.featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy: always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class\n",
    "clf = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "score = clf.score(x_test, y_test)\n",
    "print(\"Score: \", score)\n",
    "\n",
    "\n",
    "y_true = [item for sublist in y_test for item in sublist]\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_true, y_pred) # x-Achse: Actual Values, y-Achse: Predicted Values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "708a6e826013d3c53fb15234f304802a5144e9d3827d1509c4996348c0ecb0e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
